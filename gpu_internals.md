This page tracks the reference links to understand GPU internals and Inference frameworks in depth

## To understand GPU internals
1. https://www.aleksagordic.com/blog/matmul
2. https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-1-introduction
3. https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-2-using-hardware-features-to-optimize-matmul
4. https://siboehm.com/articles/22/CUDA-MMM
5. https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog


## To understand vLLM internals
1. https://www.aleksagordic.com/blog/vllm


## Tensorrt-llm references
1. https://nvidia.github.io/TensorRT-LLM/overview.html#key-capabilities
2. https://github.com/NVIDIA/TensorRT-LLM/tree/main/docs/source/blogs/tech_blog

## FlashAttention
1. https://modal.com/blog/reverse-engineer-flash-attention-4

## GPU Hardware
1. https://modal.com/blog/introducing-b200-h200
2. https://modal.com/blog/h100-and-h200-vs-b100-and-b200


## GPU Glossary
1. https://modal.com/gpu-glossary

## LLM Viz
1. https://bbycroft.net/llm


